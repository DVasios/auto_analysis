{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Thesis ~ Initial Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Types\n",
    "\n",
    " - Classification \n",
    " - Regression\n",
    " - Clustering\n",
    " - Dimension Reduction\n",
    " - Data Visualization & Analysis\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "# Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Helpers\n",
    "import pprint\n",
    "from dateutil import parser\n",
    "import ijson\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Visions | Data Type Detection\n",
    "from visions.functional import detect_type, infer_type, cast_to_inferred\n",
    "from visions.typesets import StandardSet\n",
    "from visions.typesets import CompleteSet\n",
    "\n",
    "# YData Profiling\n",
    "from ydata_profiling import ProfileReport\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Problem - Clear Objectives/Requirements\n",
    "Problems before requirements, requirements before solutions, solutions before design, and design before technology.\n",
    "\n",
    "- Problem types | Classification, Regression, Clustering, Dimension Reduction, Data Visualizations & Techniques\n",
    "- Target\n",
    "- Models | Random Forest | Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem | {Problem Type, Target Variable}\n",
    "def df_objective (problem_type, target_variable):\n",
    "\n",
    "    return {\n",
    "        'problem_type' : problem_type,\n",
    "        'target_feature' : target_variable\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering - Tabular Data\n",
    "\n",
    "**File Data Types**: json, csv, xlsx, xml, dataframe,\n",
    "\n",
    "**API**: url link, http, ftp\n",
    "\n",
    "**Category**: Web page, Image, Audio, Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Characteristics\n",
    "def file_descr (file_path): \n",
    "    descr = {\n",
    "        'FileType': os.path.splitext(file_path)[1],\n",
    "        'FileSize': os.path.getsize(file_path),\n",
    "    }\n",
    "    return descr\n",
    "\n",
    "# Converters | File type to Dataframe\n",
    "def df_convert (file_path, file_descr):\n",
    "\n",
    "    # JSON\n",
    "    if (file_descr['FileType'] == '.json'):\n",
    "\n",
    "        # Open File and Convert it to JSON Object\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = ijson.items(file, 'item')\n",
    "            json_object = []\n",
    "            count = 0\n",
    "            print(data)\n",
    "            for line in data:\n",
    "                json_object.append(line)\n",
    "                count = count + 1\n",
    "                if(count == 1000): break\n",
    "        return pd.DataFrame(json_object)\n",
    "\n",
    "    # CSV\n",
    "    elif (file_descr['FileType'] == '.csv'):\n",
    "\n",
    "        # Check whether there is a header | TODO\n",
    "        return pd.read_csv(file_path)\n",
    "    else: \n",
    "        return False\n",
    "    \n",
    "# Gather Data\n",
    "def df_gather(fp):\n",
    "\n",
    "    # File Characteristics\n",
    "    file_characteristics = file_descr(fp)\n",
    "\n",
    "    # Dataframe Initialization \n",
    "    df = df_convert(fp, file_characteristics)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Profiling\n",
    "\n",
    "### Dataset\n",
    "- Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features, Rows\n",
    "def df_shape(df):\n",
    "    return {\n",
    "        'features': df.shape[1],\n",
    "        'rows': df.shape[0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "- Role | ID, Input (Indpendant), Target (Dependant)\n",
    "- Data Type | Integer, Float, Boolean, Categorical, Complex, DateTime, Object, String\n",
    "- Feature Type | Categorical (Ordinal, Binary), Numerical\n",
    "- Level | Nominal, Interval, Ordinal, Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Type | Standard Set | Integer,  Float, Boolean, Categorical, Complex, DateTime, Object, String\n",
    "# typeset = StandardSet\n",
    "# def f_data_type(f):\n",
    "#     # f = detect_type(f, typeset)\n",
    "#     # cast_f = cast_to_inferred(f, typeset)\n",
    "#     inf_f = detect_type(f, typeset)\n",
    "#     print(inf_f)\n",
    "#     return str(inf_f)\n",
    "\n",
    "typeset = StandardSet()\n",
    "def f_data_type(f):\n",
    "    return str((detect_type(f, typeset)))\n",
    "\n",
    "# Feature Type | Categorical, Numerical, Alphanumerical | {Data Type, Unique values, Thres}\n",
    "def f_feature_type(data_type, unique_values, cat_thres=10): \n",
    "\n",
    "    if ((data_type == 'String' or \n",
    "         data_type == 'Integer' or \n",
    "         data_type == 'Float' or \n",
    "         data_type == 'Boolean') and unique_values < cat_thres):\n",
    "        return 'Categorical'\n",
    "    elif (data_type == 'Integer' or data_type == 'Float'):\n",
    "        return 'Numerical'\n",
    "    else: \n",
    "        return 'Alphanumerical'\n",
    "    \n",
    "# Qualitative Characteristics | Nominal, Interval, Ordinal, Binary\n",
    "def f_qual(data_type):\n",
    "    if (data_type == 'String'):\n",
    "        return 'Nominal'\n",
    "    elif (data_type == 'Int' or data_type == 'Float'):\n",
    "        return \n",
    "    \n",
    "# Role | Input (Independant), ID (Unique Identifier)\n",
    "def f_role(f, f_target):\n",
    "    \n",
    "    f_unique_values = len(f.unique())\n",
    "    f_total_values = len(f)\n",
    "\n",
    "    if (f_unique_values / f_total_values) > 0.9:\n",
    "        return 'id'\n",
    "    elif (f.name == f_target): \n",
    "        return 'target'\n",
    "    else:\n",
    "        return 'input' \n",
    "    \n",
    "## Feature Profiling  | {Feature, Objective}\n",
    "def f_profile(f, f_objective):\n",
    "    \n",
    "    # Data Type\n",
    "    data_type = f_data_type(f)\n",
    "\n",
    "    # Role\n",
    "    role = f_role(f, f_objective['target_feature'])\n",
    "\n",
    "    # Unique Values\n",
    "    unique_values = len(f.value_counts())\n",
    "\n",
    "    # Feature Type\n",
    "    feature_type = f_feature_type(data_type, unique_values, 10)\n",
    "\n",
    "    return {\n",
    "        'data_type' : data_type,\n",
    "        'role' : role,\n",
    "        'unique_values' : unique_values,\n",
    "        'feature_type' : feature_type\n",
    "    }\n",
    "\n",
    "## All feature profiling\n",
    "def df_profile(df, df_objective):\n",
    "    df_profile = {}\n",
    "\n",
    "    # Dataset\n",
    "    df_profile['dataset'] = df_shape(df)\n",
    "\n",
    "    # Target Feature\n",
    "    df_profile['target_feature'] = df_objective['target_feature']\n",
    "\n",
    "    # Features\n",
    "    df_features = {}\n",
    "    for f in df.columns: \n",
    "        df_features[f] = f_profile(df[f], df_objective)\n",
    "    df_profile['features'] = df_features\n",
    "\n",
    "    return df_profile "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Profiling | Samples Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# House Prices\n",
    "# hp_profiling = df_profile(df_hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modcloth\n",
    "# md_profiling = df_profile(df_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis & Visualization | Descriptive Statistics\n",
    "\n",
    "### Dataset\n",
    "- Duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Duplicates | Check if they exist\n",
    "def df_duplicates(df):\n",
    "    return {\n",
    "        'exist' : df.duplicated().any(),\n",
    "        'sum' : df.duplicated().sum()\n",
    "    } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis \n",
    "\n",
    "- Interval, Ordinal Statistics | Count, Mean,  Std, Min, 25%, 50%, 75%, Max\n",
    "- Missing Values \n",
    "- Outliers\n",
    "- Histogram\n",
    "- Box Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "def f_statistics(f, d_type, f_type):\n",
    "\n",
    "    f_statistics = {}\n",
    "    if f_type == 'Numerical' and d_type != 'String':\n",
    "\n",
    "        descr = f.describe()\n",
    "        f_statistics = {\n",
    "            'count' : int(descr['count']),\n",
    "            'mean' : round(descr['mean'], 2),\n",
    "            'std' : round(descr['std'], 2),\n",
    "            'min' : round(descr['min'], 2),\n",
    "            'max' : round(descr['max'], 2)\n",
    "        }\n",
    "    elif f_type == 'Categorical': \n",
    "        for i, y in f.value_counts().items():\n",
    "            \n",
    "            # Calculate Frequency of each categorical value\n",
    "            freq = round(y / len(f), 2)\n",
    "            f_statistics[i] = {\n",
    "                'value': y,\n",
    "                'frequency': freq\n",
    "            }\n",
    "\n",
    "    return f_statistics\n",
    "\n",
    "# Missing Values | Return: {Total Missing Values, Percentage}\n",
    "def f_missing_data(f):\n",
    "\n",
    "    # Null Values\n",
    "    null_values = f.isnull().sum() \n",
    "\n",
    "    # Empty Values \n",
    "    empty_values = f.isin(['']).sum()\n",
    "\n",
    "    # Missing Values\n",
    "    missing_values = null_values + empty_values\n",
    "\n",
    "    ## Percentage\n",
    "    percentage = round((missing_values / len(f)), 2)\n",
    "    return {\n",
    "        'missing_values': missing_values,\n",
    "        'percentage': percentage\n",
    "    }\n",
    "\n",
    "## Univariate | Feature | {Dataframe_Feature, Data Type, Feature Type}\n",
    "def f_univariate(f, d_type, f_type): \n",
    "    f_univariate = {}\n",
    "\n",
    "    # Statistics\n",
    "    f_univariate['statistics'] = f_statistics(f, d_type, f_type)\n",
    "\n",
    "    # Missing Values\n",
    "    f_univariate['missing_data'] = f_missing_data(f)\n",
    "\n",
    "    return f_univariate\n",
    "\n",
    "# Univariate | Dataframe | {Dataframe, Dataframe_Profiling}\n",
    "def df_univariate(df, df_prof):\n",
    "\n",
    "    df_univariate = {}\n",
    "    for f, d in df_prof['features'].items():\n",
    "        df_univariate[f] = f_univariate(df[f], d['data_type'], d['feature_type'])\n",
    "\n",
    "    df_univariate['features'] = df_univariate\n",
    "\n",
    "    return df_univariate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Analysis\n",
    " - How each variable correlates to target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis | Each feature with target variable\n",
    "def f_corr (f, corr_matrix):\n",
    "    if f in corr_matrix:\n",
    "        return round(corr_matrix[f], 2)\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "# Bivariate Analysis | Dataframe\n",
    "def df_bivariate(df, df_prof):\n",
    "\n",
    "    df_bivariate = {}\n",
    "\n",
    "    # Correlation Analysis\n",
    "    corr_matrix = df.corr(numeric_only=True)[df_prof['target_feature']]\n",
    "    for f in df_prof['features']:\n",
    "        df_bivariate[f] = f_corr(f, corr_matrix)\n",
    "    \n",
    "    return df_bivariate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Analysis\n",
    "- Normality -> Data should look like normal distribution\n",
    "- Homoscedasticity -> \n",
    "- Linearity -> Linear Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA  | {Datframe, Dataframe Profiling}\n",
    "def df_eda (df, df_prof):\n",
    "\n",
    "    df_eda = {}\n",
    "\n",
    "    # Duplicates\n",
    "    df_eda['duplicates'] = df_duplicates(df)\n",
    "\n",
    "    # Univariate\n",
    "    df_eda['univariate'] = df_univariate(df, df_prof)\n",
    "\n",
    "    # Bivariate\n",
    "    df_eda['bivariate'] = df_bivariate(df, df_prof)\n",
    " \n",
    "    return  df_eda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "Dataset description in one dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Description\n",
    "def df_describe(df, df_objective):\n",
    "\n",
    "    # Profile \n",
    "    df_prof = df_profile(df, df_objective)\n",
    "\n",
    "    # Statistics\n",
    "    df_stat = df_eda(df, df_prof)\n",
    "\n",
    "    # Duplicates\n",
    "    df_prof['dataset']['duplicates'] = df_stat['duplicates']\n",
    "\n",
    "    # Univariate\n",
    "    for f, i in df_prof['features'].items():\n",
    "        df_prof['features'][f]['eda'] = df_stat['univariate'][f]\n",
    "\n",
    "    return df_prof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning & Correcting & Formatting & Completing (Transform)\n",
    "\n",
    "### Drop Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning \n",
    "- Data Imputation ~ Handle Missing Values ~ Methods: Mean, Median, KNN, Most Frequent Value, Random Numbers between mean & std, Exploit correlated feature(s)\n",
    "- Data Anomaly Detection ~ Handle Outliers ~ Interquartile Range (IQR) method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Duplicates\n",
    "def df_drop_duplicates(df):\n",
    "\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "# Drop Features | Duplicates, IDs, Missing Values > Thres \n",
    "def df_drop(df, df_descr, drop_perc = 0.9): \n",
    "\n",
    "    # Drop Features with missing percentage > thres\n",
    "    for f, details in df_descr['features'].items():\n",
    "\n",
    "        # If missing values are above threshold | Drop\n",
    "        if (details['eda']['missing_data']['percentage'] >= drop_perc):\n",
    "            df = df.drop(columns=[f])\n",
    "    \n",
    "    # Drop IDs\n",
    "    columns = []\n",
    "    for f, details in df_descr['features'].items():\n",
    "        if (details['role'] == 'id'):\n",
    "            columns.append(f)\n",
    "    df = df.drop(columns=columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Data Imputation | {Dataframe, Profile, EDA}\n",
    "def df_impute(df: pd.DataFrame, df_descr: dict):\n",
    "\n",
    "    # Split Categorical and Numerical Columns\n",
    "    numerical_columns = []\n",
    "    categorical_columns = []\n",
    "\n",
    "    # For each feature\n",
    "    for f, p in df_descr['features'].items():\n",
    "\n",
    "        # Split | If missing percentage is above 0\n",
    "        if (p['eda']['missing_data']['missing_values'] > 0.0):\n",
    "            if (p['feature_type'] == 'Numerical'):\n",
    "                numerical_columns.append(f)\n",
    "            elif (p['feature_type'] == 'Categorical'):\n",
    "                categorical_columns.append(f)\n",
    "\n",
    "    # Imputers\n",
    "    numerical_imputer = SimpleImputer(strategy='mean')\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "    # Mean | Numerical\n",
    "    if (len(numerical_columns) > 0):\n",
    "        df[numerical_columns] = numerical_imputer.fit_transform(df[numerical_columns])\n",
    "\n",
    "    # Mode | Categorical\n",
    "    if (len(categorical_columns) > 0):\n",
    "        df[categorical_columns] = categorical_imputer.fit_transform(df[categorical_columns])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Handle Outliers | {Dataframe, Profile, EDA, Threshold}\n",
    "def df_handle_outliers(df : pd.DataFrame, df_profile : dict, df_eda : dict, iqr_thres : float = 1.5):\n",
    "\n",
    "    # Numerical Columns\n",
    "    numerical_columns = []\n",
    "\n",
    "    for f, p in df_profile['features'].items():\n",
    "        if (p['feature_type'] == 'Numerical'):\n",
    "            numerical_columns.append(f)\n",
    "\n",
    "    for f in numerical_columns: \n",
    "        Q1 = df[f].quantile(0.25)\n",
    "        Q3 = df[f].quantile(0.75)\n",
    "\n",
    "        IQR = Q3 - Q1 \n",
    "        outliers = ((df[f] < (Q1 - iqr_thres * IQR)) | (df[f] > (Q3 + iqr_thres * IQR)))\n",
    "        df = df[~outliers]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Dataframe Clean | {Dataframe, Description, MV Thres, Outlier Thres}\n",
    "def df_clean (df, df_descr, df_objective, mv_thres = 0.9, outlier_thres = 1.5):\n",
    "\n",
    "    # Drop Duplicates\n",
    "    df_duplic = df_drop_duplicates(df)\n",
    "\n",
    "    # Drop Features\n",
    "    df_dropped = df_drop(df_duplic, df_descr, mv_thres)\n",
    "\n",
    "    ## Describe\n",
    "    df_descr_dropped = df_describe(df_dropped, df_objective)\n",
    "\n",
    "    # # Data Imputation | 0.5 %\n",
    "    df_imputed = df_impute(df_dropped, df_descr_dropped)\n",
    "    return df_imputed\n",
    "\n",
    "    # ## Describe\n",
    "    # df_descr_imputed = df_describe(df_imputed, df_objective)\n",
    "\n",
    "    # # Drop Outliers | 1.5\n",
    "    # df_cleaned = df_handle_outliers(df_imputed, df_descr_imputed, outlier_thres)\n",
    "\n",
    "    # return df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "- Feature Encoding \n",
    "- Feature Scaling\n",
    "- Feature Extraction\n",
    "- Feature Selection | Either in data-preprocessing, or the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Encoding | {Feature, Encoding Type}\n",
    "def f_encode(f, f_name, encode_type = 'one-hot'):\n",
    "\n",
    "    if (encode_type == 'one-hot'):\n",
    "\n",
    "        one_hot_encoder = pd.get_dummies(f)\n",
    "        one_hot_encoder = one_hot_encoder.astype(int)\n",
    "\n",
    "        return one_hot_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Selection | Remove features with low or zero variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering | {Dataframe, Profile, EDA}\n",
    "def df_feature_eng(df, df_descr): \n",
    "\n",
    "    # Feature Encoding for categorical variables\n",
    "    for f, details in df_descr['features'].items():\n",
    "        if (details['feature_type'] == 'Categorical' and \n",
    "            details['role'] != 'id' and \n",
    "            details['role'] != 'target' and \n",
    "            details['data_type'] != 'Integer' and\n",
    "            details['data_type'] != 'Float'):\n",
    "\n",
    "            # Encode\n",
    "            f_encoded = f_encode(df[f], f, 'one-hot')\n",
    "            \n",
    "            # Change Names of encoded df\n",
    "\n",
    "\n",
    "            # Concat to existing df\n",
    "            df = df.drop(columns=[f])\n",
    "            df = pd.concat([df, f_encoded], axis=1)\n",
    "\n",
    "    # Feature \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated Data Preprocessing | {File Source, Problem Type, Target}\n",
    "def auto_preproc (file_source, problem_type, target, params = {}):\n",
    "\n",
    "    # Gather\n",
    "    df = df_gather(file_source)\n",
    "\n",
    "    # Objective\n",
    "    df_object = df_objective(problem_type, target)\n",
    "\n",
    "    # Describe\n",
    "    df_descr = df_describe(df, df_object)\n",
    "\n",
    "    # Clean\n",
    "    df_cleaned = df_clean(df, df_descr, df_object)\n",
    "\n",
    "    # Describe | Cleaned\n",
    "    df_descr_cleaned = df_describe(df_cleaned, df_object)\n",
    "\n",
    "    # Feature Engineer\n",
    "    df_f_eng = df_feature_eng(df_cleaned, df_descr_cleaned)\n",
    "\n",
    "    # Describe | Feature Engineered\n",
    "    # df_descr_f_eng = df_describe(df_f_eng, df_object)\n",
    "\n",
    "    return {\n",
    "        'df' : df,\n",
    "        'descr' : df_descr, \n",
    "        'cleaned' : df_cleaned, \n",
    "        'descr_cleaned' : df_descr_cleaned, \n",
    "        'f_eng' : df_f_eng, \n",
    "        # 'descr_f_eng' : df_descr_f_eng \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Data Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_yajl2.items object at 0x7f4ae66221f0>\n"
     ]
    }
   ],
   "source": [
    "# Titanic\n",
    "# tn = auto_preproc('data/sample_data/titanic/train.csv', 'classification', 'Survived')\n",
    "\n",
    "# House Prices\n",
    "# hp = auto_preproc('data/sample_data/house_price_data/train.csv', 'classification', 'SalePrice')\n",
    "\n",
    "# Wasabi\n",
    "ws = auto_preproc('data/sample_data/Wasabi/batterytesters_dataset.json', 'classification', 'commonalarm')\n",
    "\n",
    "# Credit Fraud\n",
    "# df_cc = df_gather('data/sample_data/credit_card_fraud/creditcard.csv')\n",
    "\n",
    "# Modcloth | TOCHECK\n",
    "# file_path_md = 'data/sample_data/modcloth/modcloth_final_data.json'\n",
    "# df_md = pd.read_json(file_path_md, lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_df = df_gather('data/sample_data/Wasabi/batterytesters_dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_type': 'Boolean',\n",
       " 'role': 'target',\n",
       " 'unique_values': 1,\n",
       " 'feature_type': 'Categorical',\n",
       " 'eda': {'statistics': {False: {'value': 1000, 'frequency': 1.0}},\n",
       "  'missing_data': {'missing_values': 0, 'percentage': 0.0}}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws['descr']['features']['commonalarm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>btname</th>\n",
       "      <th>commonalarm</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B5</th>\n",
       "      <th>B9</th>\n",
       "      <th>...</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1687625779795</td>\n",
       "      <td>DCE2</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1687625779795</td>\n",
       "      <td>LC2</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1687625779795</td>\n",
       "      <td>LC3</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1687625779795</td>\n",
       "      <td>LC4</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1687625779795</td>\n",
       "      <td>LC5</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1687627812881</td>\n",
       "      <td>2CH1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1687627812881</td>\n",
       "      <td>2CH2</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1687627812881</td>\n",
       "      <td>2CH3</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1687627812881</td>\n",
       "      <td>2CH4</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1687627812881</td>\n",
       "      <td>BT2</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          datetime btname  commonalarm  False  True     B1  B2  B5  B9  ...  \\\n",
       "0    1687625779795   DCE2        False      0     1  1   0   0   0   0  ...   \n",
       "1    1687625779795    LC2        False      0     1  0   0   0   0   0  ...   \n",
       "2    1687625779795    LC3        False      0     1  0   0   1   0   0  ...   \n",
       "3    1687625779795    LC4        False      0     1  1   0   0   0   0  ...   \n",
       "4    1687625779795    LC5        False      0     1  0   0   0   1   0  ...   \n",
       "..             ...    ...          ...    ...   ... ..  ..  ..  ..  ..  ...   \n",
       "995  1687627812881   2CH1        False      0     1  0   0   0   0   1  ...   \n",
       "996  1687627812881   2CH2        False      0     1  0   0   0   0   1  ...   \n",
       "997  1687627812881   2CH3        False      0     1  0   0   0   0   0  ...   \n",
       "998  1687627812881   2CH4        False      0     1  0   0   0   0   0  ...   \n",
       "999  1687627812881    BT2        False      1     0  0   1   0   0   0  ...   \n",
       "\n",
       "     False  True  False  True  False  True  False  True  False  True  \n",
       "0        0     1      1     0      1     0      0     1      1     0  \n",
       "1        1     0      0     1      0     1      0     1      1     0  \n",
       "2        1     0      0     1      0     1      0     1      0     1  \n",
       "3        1     0      1     0      1     0      0     1      0     1  \n",
       "4        1     0      0     1      0     1      0     1      0     1  \n",
       "..     ...   ...    ...   ...    ...   ...    ...   ...    ...   ...  \n",
       "995      1     0      0     1      0     1      1     0      0     1  \n",
       "996      1     0      0     1      0     1      1     0      0     1  \n",
       "997      1     0      0     1      0     1      0     1      0     1  \n",
       "998      1     0      0     1      0     1      0     1      0     1  \n",
       "999      0     1      0     1      0     1      0     1      0     1  \n",
       "\n",
       "[1000 rows x 21 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws['f_eng']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "for f, d in hp_descr_2['features'].items():\n",
    "\n",
    "    if (d['eda']['missing_data']['percentage'] > 0.0):\n",
    "        print('yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
