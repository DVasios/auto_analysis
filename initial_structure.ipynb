{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Thesis ~ Initial Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Types\n",
    "\n",
    " - Classification \n",
    " - Regression\n",
    " - Clustering\n",
    " - Dimension Reduction\n",
    " - Data Visualization & Analysis\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "# Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Helpers\n",
    "import ijson\n",
    "from datetime import datetime\n",
    "\n",
    "# Visions | Data Type Detection\n",
    "from visions.functional import detect_type\n",
    "from visions.typesets import StandardSet\n",
    "import visions\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "def has_missing_data(df_descr):\n",
    "    for f, d in df_descr['features'].items():\n",
    "\n",
    "        if (d['eda']['missing_data']['percentage'] > 0.0):\n",
    "            print('yes')\n",
    "            print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Problem - Clear Objectives/Requirements\n",
    "Problems before requirements, requirements before solutions, solutions before design, and design before technology.\n",
    "\n",
    "- Problem types | Classification, Regression, Clustering, Dimension Reduction, Data Visualizations & Techniques\n",
    "- Target\n",
    "- Models | Random Forest | Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem | {Problem Type, Target Variable}\n",
    "def df_objective (problem_type, target_variable):\n",
    "\n",
    "    return {\n",
    "        'problem_type' : problem_type,\n",
    "        'target_feature' : target_variable\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering - Tabular Data\n",
    "\n",
    "**File Data Types**: json, csv, xlsx, xml, dataframe,\n",
    "\n",
    "**API**: url link, http, ftp\n",
    "\n",
    "**Category**: Web page, Image, Audio, Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Characteristics\n",
    "def file_descr (file_path): \n",
    "    descr = {\n",
    "        'FileType': os.path.splitext(file_path)[1],\n",
    "        'FileSize': os.path.getsize(file_path),\n",
    "    }\n",
    "    return descr\n",
    "\n",
    "# Converters | File type to Dataframe\n",
    "def df_convert (file_path, file_descr, nrows = 10000):\n",
    "\n",
    "    # JSON\n",
    "    if (file_descr['FileType'] == '.json'):\n",
    "\n",
    "        # Open File and Convert it to JSON Object\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = ijson.items(file, 'item')\n",
    "            json_object = []\n",
    "            count = 0\n",
    "            for line in data:\n",
    "                json_object.append(line)\n",
    "                count = count + 1\n",
    "                if(count == 1000): break\n",
    "        return pd.DataFrame(json_object)\n",
    "\n",
    "    # CSV\n",
    "    elif (file_descr['FileType'] == '.csv'):\n",
    "\n",
    "        # Check whether there is a header | TODO\n",
    "        return pd.read_csv(file_path, nrows=nrows)\n",
    "    else: \n",
    "        return False\n",
    "    \n",
    "# Gather Data\n",
    "def df_gather(fp):\n",
    "\n",
    "    # File Characteristics\n",
    "    file_characteristics = file_descr(fp)\n",
    "\n",
    "    # Dataframe Initialization \n",
    "    df = df_convert(fp, file_characteristics)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Profiling\n",
    "\n",
    "### Dataset\n",
    "- Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features, Rows\n",
    "def df_shape(df):\n",
    "    return {\n",
    "        'features': df.shape[1],\n",
    "        'rows': df.shape[0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "- Role | ID, Input (Indpendant), Target (Dependant)\n",
    "- Data Type | Integer, Float, Boolean, Categorical, Complex, DateTime, Object, String\n",
    "- Feature Type | Categorical (Ordinal, Binary), Numerical\n",
    "- Level | Nominal, Interval, Ordinal, Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Type | Standard Set | Integer,  Float, Boolean, Categorical, Complex, DateTime, Object, String\n",
    "typeset = StandardSet()\n",
    "def f_data_type(f : pd.DataFrame):\n",
    "\n",
    "    return str(typeset.infer_type(f.astype(str)))\n",
    "\n",
    "# Feature Type | Categorical, Numerical, Alphanumerical | {Data Type, Unique values Ratio, Thres}\n",
    "def f_feature_type(data_type, unique_values_ratio, thres = 0.1): \n",
    "\n",
    "    if ((data_type == 'String' or \n",
    "         data_type == 'Integer' or \n",
    "         data_type == 'Float' or \n",
    "         data_type == 'Boolean') and unique_values_ratio < thres):\n",
    "        return 'Categorical'\n",
    "    elif (data_type == 'Integer' or data_type == 'Float'):\n",
    "        return 'Numerical'\n",
    "    else: \n",
    "        return 'Alphanumerical'\n",
    "    \n",
    "# Qualitative Characteristics | Nominal, Interval, Ordinal, Binary\n",
    "def f_qual(data_type):\n",
    "    if (data_type == 'String'):\n",
    "        return 'Nominal'\n",
    "    elif (data_type == 'Int' or data_type == 'Float'):\n",
    "        return \n",
    "    \n",
    "# Role | Input (Independant), ID (Unique Identifier)\n",
    "def f_role(f, f_target):\n",
    "    \n",
    "    f_unique_values = len(f.unique())\n",
    "    f_total_values = len(f)\n",
    "\n",
    "    if (f_unique_values / f_total_values) > 0.9:\n",
    "        return 'id'\n",
    "    elif (f.name == f_target): \n",
    "        return 'target'\n",
    "    else:\n",
    "        return 'input' \n",
    "    \n",
    "## Feature Profiling  | {Feature, Objective, Unique Values Threshold}\n",
    "def f_profile(f, f_objective, unique_values_thres):\n",
    "    \n",
    "    # Data Type\n",
    "    data_type = f_data_type(f)\n",
    "\n",
    "    # Role\n",
    "    role = f_role(f, f_objective['target_feature'])\n",
    "\n",
    "    # Unique Values Ration\n",
    "    unique_values = len(f.value_counts()) \n",
    "    unique_values_ratio = unique_values / f.count()\n",
    "\n",
    "    # Feature Type\n",
    "    feature_type = f_feature_type(data_type, unique_values_ratio, unique_values_thres)\n",
    "\n",
    "    return {\n",
    "        'data_type' : data_type,\n",
    "        'role' : role,\n",
    "        'unique_values' : unique_values,\n",
    "        'feature_type' : feature_type\n",
    "    }\n",
    "\n",
    "## All feature profiling\n",
    "def df_profile(df, df_objective, unique_values_ratio):\n",
    "    df_profile = {}\n",
    "\n",
    "    # Dataset\n",
    "    df_profile['dataset'] = df_shape(df)\n",
    "\n",
    "    # Target Feature\n",
    "    df_profile['target_feature'] = df_objective['target_feature']\n",
    "\n",
    "    # Features\n",
    "    df_features = {}\n",
    "    for f in df.columns: \n",
    "        df_features[f] = f_profile(df[f], df_objective, unique_values_ratio)\n",
    "    df_profile['features'] = df_features\n",
    "\n",
    "    return df_profile "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis & Visualization | Descriptive Statistics\n",
    "\n",
    "### Dataset\n",
    "- Duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Duplicates | Check if they exist\n",
    "def df_duplicates(df):\n",
    "    return {\n",
    "        'exist' : df.duplicated().any(),\n",
    "        'sum' : df.duplicated().sum()\n",
    "    } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis \n",
    "\n",
    "- Interval, Ordinal Statistics | Count, Mean,  Std, Min, 25%, 50%, 75%, Max\n",
    "- Missing Values \n",
    "- Outliers\n",
    "- Histogram\n",
    "- Box Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "def f_statistics(f, d_type, f_type):\n",
    "\n",
    "    f_statistics = {}\n",
    "    if f_type == 'Numerical' and d_type != 'String':\n",
    "\n",
    "        descr = f.describe()\n",
    "        f_statistics = {\n",
    "            'count' : int(descr['count']),\n",
    "            'mean' : round(descr['mean'], 2),\n",
    "            'std' : round(descr['std'], 2),\n",
    "            'min' : round(descr['min'], 2),\n",
    "            'max' : round(descr['max'], 2)\n",
    "        }\n",
    "    elif f_type == 'Categorical': \n",
    "        for i, y in f.value_counts().items():\n",
    "            \n",
    "            # Calculate Frequency of each categorical value\n",
    "            freq = round(y / len(f), 2)\n",
    "            f_statistics[i] = {\n",
    "                'value': y,\n",
    "                'frequency': freq\n",
    "            }\n",
    "\n",
    "    return f_statistics\n",
    "\n",
    "# Missing Values | Return: {Total Missing Values, Percentage}\n",
    "def f_missing_data(f):\n",
    "\n",
    "    # Null Values\n",
    "    null_values = f.isnull().sum() \n",
    "\n",
    "    # Empty Values \n",
    "    empty_values = f.isin(['']).sum()\n",
    "\n",
    "    # Missing Values\n",
    "    missing_values = null_values + empty_values\n",
    "\n",
    "    ## Percentage\n",
    "    percentage = round((missing_values / len(f)), 2)\n",
    "    return {\n",
    "        'missing_values': missing_values,\n",
    "        'percentage': percentage\n",
    "    }\n",
    "\n",
    "## Univariate | Feature | {Dataframe_Feature, Data Type, Feature Type}\n",
    "def f_univariate(f, d_type, f_type): \n",
    "    f_univariate = {}\n",
    "\n",
    "    # Statistics\n",
    "    f_univariate['statistics'] = f_statistics(f, d_type, f_type)\n",
    "\n",
    "    # Missing Values\n",
    "    f_univariate['missing_data'] = f_missing_data(f)\n",
    "\n",
    "    return f_univariate\n",
    "\n",
    "# Univariate | Dataframe | {Dataframe, Dataframe_Profiling}\n",
    "def df_univariate(df, df_prof):\n",
    "\n",
    "    df_univariate = {}\n",
    "    for f, d in df_prof['features'].items():\n",
    "        df_univariate[f] = f_univariate(df[f], d['data_type'], d['feature_type'])\n",
    "\n",
    "    df_univariate['features'] = df_univariate\n",
    "\n",
    "    return df_univariate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Analysis\n",
    " - How each variable correlates to target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis | Each feature with target variable\n",
    "def f_corr (f, corr_matrix):\n",
    "    if f in corr_matrix:\n",
    "        return round(corr_matrix[f], 2)\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "# Bivariate Analysis | Dataframe\n",
    "def df_bivariate(df, df_prof):\n",
    "\n",
    "    # Check if target feature is in String format | Label Encoding\n",
    "    if (df_prof['features'][df_prof['target_feature']]['data_type'] == 'String'):\n",
    "\n",
    "        label_encoder = LabelEncoder()\n",
    "        df[df_prof['target_feature']] = label_encoder.fit_transform(df[df_prof['target_feature']])\n",
    "\n",
    "    df_bivariate = {}\n",
    "\n",
    "    # Correlation Analysis\n",
    "    corr_matrix = df.corr(numeric_only=True)[df_prof['target_feature']]\n",
    "    for f in df_prof['features']:\n",
    "        df_bivariate[f] = f_corr(f, corr_matrix)\n",
    "    \n",
    "    return df_bivariate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Analysis\n",
    "- Normality -> Data should look like normal distribution\n",
    "- Homoscedasticity -> \n",
    "- Linearity -> Linear Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA  | {Datframe, Dataframe Profiling}\n",
    "def df_eda (df, df_prof):\n",
    "\n",
    "    df_eda = {}\n",
    "\n",
    "    # Duplicates\n",
    "    df_eda['duplicates'] = df_duplicates(df)\n",
    "\n",
    "    # Univariate\n",
    "    df_eda['univariate'] = df_univariate(df, df_prof)\n",
    "\n",
    "    # Bivariate\n",
    "    df_eda['bivariate'] = df_bivariate(df, df_prof)\n",
    " \n",
    "    return  df_eda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "Dataset description in one dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Description | {Dataframe, Objective, Unique Values Ratio}\n",
    "def df_describe(df, df_objective, unique_values_ratio):\n",
    "\n",
    "    # Profile \n",
    "    df_prof = df_profile(df, df_objective, unique_values_ratio)\n",
    "\n",
    "    # Statistics\n",
    "    df_stat = df_eda(df, df_prof)\n",
    "\n",
    "    # Duplicates\n",
    "    df_prof['dataset']['duplicates'] = df_stat['duplicates']\n",
    "\n",
    "    # Univariate\n",
    "    for f, i in df_prof['features'].items():\n",
    "        df_prof['features'][f]['eda'] = df_stat['univariate'][f]\n",
    "\n",
    "    return df_prof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning & Correcting & Formatting & Completing (Transform)\n",
    "\n",
    "### Drop Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning \n",
    "- Data Imputation ~ Handle Missing Values ~ Methods: Mean, Median, KNN, Most Frequent Value, Random Numbers between mean & std, Exploit correlated feature(s)\n",
    "- Data Anomaly Detection ~ Handle Outliers ~ Interquartile Range (IQR) method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Duplicates\n",
    "def df_drop_duplicates(df):\n",
    "\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "# Drop Features | IDs, Missing Values > Thres \n",
    "def df_drop(df, df_descr, drop_perc = 0.9): \n",
    "\n",
    "    # Drop Features with missing percentage > thres\n",
    "    for f, details in df_descr['features'].items():\n",
    "\n",
    "        # If missing values are above threshold | Drop\n",
    "        if (details['eda']['missing_data']['percentage'] >= drop_perc):\n",
    "            df = df.drop(columns=[f])\n",
    "    \n",
    "    # Drop IDs\n",
    "    # columns = []\n",
    "    # for f, details in df_descr['features'].items():\n",
    "    #     if (details['role'] == 'id'):\n",
    "    #         columns.append(f)\n",
    "    # df = df.drop(columns=columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Data Imputation | {Dataframe, Profile, EDA}\n",
    "def df_impute(df: pd.DataFrame, df_descr: dict):\n",
    "\n",
    "    # Split Categorical and Numerical Columns\n",
    "    numerical_columns = []\n",
    "    categorical_columns = []\n",
    "    alphanumerical_columns = []\n",
    "\n",
    "    # For each feature\n",
    "    for f, p in df_descr['features'].items():\n",
    "\n",
    "        # Split | If missing percentage is above 0\n",
    "        if (p['eda']['missing_data']['missing_values'] > 0.0):\n",
    "            if (p['feature_type'] == 'Numerical'):\n",
    "                numerical_columns.append(f)\n",
    "            elif (p['feature_type'] == 'Categorical'):\n",
    "                categorical_columns.append(f)\n",
    "            elif (p['feature_type'] == 'Alphanumerical'):\n",
    "                alphanumerical_columns.append(f)\n",
    "\n",
    "    # Imputers\n",
    "    numerical_imputer = SimpleImputer(strategy='mean')\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "    # Mean | Numerical\n",
    "    if (len(numerical_columns) > 0):\n",
    "        df[numerical_columns] = numerical_imputer.fit_transform(df[numerical_columns])\n",
    "\n",
    "    # Mode | Categorical\n",
    "    if (len(categorical_columns) > 0):\n",
    "        df[categorical_columns] = categorical_imputer.fit_transform(df[categorical_columns])\n",
    "\n",
    "    # Empty Space | Alphanumerical\n",
    "    if (len(alphanumerical_columns) > 0):\n",
    "        df[alphanumerical_columns] = df[alphanumerical_columns].fillna('')\n",
    "    \n",
    "    df = df.fillna('')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Handle Outliers | {Dataframe, Profile, EDA, Threshold}\n",
    "def df_handle_outliers(df : pd.DataFrame, df_descr : dict,  iqr_thres : float = 1.5):\n",
    "\n",
    "    # Numerical Columns\n",
    "    numerical_columns = []\n",
    "\n",
    "    for f, p in df_descr['features'].items():\n",
    "        if (p['feature_type'] == 'Numerical'):\n",
    "            numerical_columns.append(f)\n",
    "\n",
    "    for f in numerical_columns: \n",
    "        Q1 = df[f].quantile(0.25)\n",
    "        Q3 = df[f].quantile(0.75)\n",
    "\n",
    "        IQR = Q3 - Q1 \n",
    "        outliers = ((df[f] < (Q1 - iqr_thres * IQR)) | (df[f] > (Q3 + iqr_thres * IQR)))\n",
    "        df = df[~outliers]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Dataframe Clean | {Dataframe, Description, MV Thres, Outlier Thres, Unique Value Thres}\n",
    "def df_clean (df, df_descr, df_objective, mv_thres = 0.9, outlier_thres = 1.5, uv_thres = 0.1):\n",
    "\n",
    "    # Drop Duplicates\n",
    "    df_duplic = df_drop_duplicates(df)\n",
    "\n",
    "    # Drop Features\n",
    "    df_dropped = df_drop(df_duplic, df_descr, mv_thres)\n",
    "\n",
    "    ## Describe\n",
    "    df_descr_dropped = df_describe(df_dropped, df_objective, uv_thres)\n",
    "\n",
    "    # Data Imputation | 0.5 %\n",
    "    df_imputed = df_impute(df_dropped, df_descr_dropped)\n",
    "\n",
    "    ## Describe\n",
    "    df_descr_imputed = df_describe(df_imputed, df_objective, uv_thres)\n",
    "\n",
    "    # Drop Outliers | 1.5\n",
    "    # df_cleaned = df_handle_outliers(df_imputed, df_descr_imputed, outlier_thres)\n",
    "\n",
    "    # return df_cleaned\n",
    "    return df_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "- Feature Extraction\n",
    "- Feature Encoding \n",
    "- Feature Scaling\n",
    "- Feature Selection | Either in data-preprocessing, or the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Extraction | {Series, Extraction Type, Frequency Threshold}\n",
    "def f_extract(f, type = 'TFID', freq_thres = 0.1):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    x_tfidf = tfidf_vectorizer.fit_transform(f.values.astype('U'))\n",
    "\n",
    "    f_extracted = pd.DataFrame(x_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "    # Keep specific columns\n",
    "    term_document_frequency = (f_extracted > 0).sum(axis=0)\n",
    "    term_document_frequency_ratio = term_document_frequency / f.count()\n",
    "\n",
    "    columns_to_keep = term_document_frequency[term_document_frequency_ratio >= freq_thres].index\n",
    "\n",
    "    f_extracted_filtered = f_extracted[columns_to_keep]\n",
    "\n",
    "    # Change Names of encoded df\n",
    "    for c in f_extracted_filtered.columns: \n",
    "        name = str(f.name) + '_' + str(c)\n",
    "        f_extracted_filtered = f_extracted_filtered.rename(columns={c : name})\n",
    "    \n",
    "    return f_extracted_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Encoding | {Feature, Encoding Type}\n",
    "def f_encode(f, f_type, f_data_type, encode_type = 'one-hot'):\n",
    "\n",
    "    # DateTime\n",
    "    if (f_data_type  == 'DateTime'):\n",
    "        f_encoded = pd.DataFrame()\n",
    "\n",
    "        # DateTime Encode\n",
    "        f = pd.to_datetime(f)\n",
    "        print(f.dt)\n",
    "        f_encoded['year'] = f.dt.year\n",
    "        f_encoded['month'] = f.dt.month\n",
    "        f_encoded['day'] = f.dt.day\n",
    "\n",
    "    # Categorical\n",
    "    if (f_type == 'Categorical' and encode_type == 'one-hot'):\n",
    "\n",
    "        f_encoded = pd.get_dummies(f)\n",
    "        f_encoded = f_encoded.astype(int)\n",
    "    \n",
    "    # Change Names of encoded df\n",
    "    for c in f_encoded.columns: \n",
    "        name = str(f.name) + '_' + str(c)\n",
    "        f_encoded = f_encoded.rename(columns={c : name})\n",
    "\n",
    "    return f_encoded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Selection | Remove features with low or zero variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Feature Scaling\n",
    "# def f_scaling(f, scaling_type):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering | {Dataframe, Profile, EDA}\n",
    "def df_feature_eng(df, df_descr): \n",
    "\n",
    "    # Feature Extraction\n",
    "    col_remove = []\n",
    "    for f, details in df_descr['features'].items():\n",
    "\n",
    "        # Extract Text Features\n",
    "        if (details['feature_type'] == 'Alphanumerical' and\n",
    "            details['role'] != 'target'):\n",
    "\n",
    "            f_extracted = f_extract(df[f])\n",
    "\n",
    "            # Concat to existing df\n",
    "            df = df.drop(columns=[f])\n",
    "            df = pd.concat([df, f_extracted], axis=1)\n",
    "\n",
    "            # Append\n",
    "            col_remove.append(f)\n",
    "\n",
    "    # Remove from description\n",
    "    for i in col_remove:\n",
    "        del df_descr['features'][i]\n",
    "        \n",
    "    # Feature Encoding\n",
    "    for f, details in df_descr['features'].items():\n",
    "        if ((details['feature_type'] == 'Categorical' or details['data_type'] == 'DateTime') and \n",
    "            details['role'] != 'id' and \n",
    "            details['role'] != 'target' and \n",
    "            details['data_type'] != 'Integer' and\n",
    "            details['data_type'] != 'Float'):\n",
    "\n",
    "            # Encode\n",
    "            if( f=='YearBuilt'):\n",
    "                print(df[f])\n",
    "            f_encoded = f_encode(df[f], details['feature_type'], details['data_type'], 'one-hot')\n",
    "\n",
    "            # Concat to existing df\n",
    "            df = df.drop(columns=[f])\n",
    "            df = pd.concat([df, f_encoded], axis=1)\n",
    "\n",
    "    # Feature Selection \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated Data Preprocessing | {File Source, Problem Type, Target}\n",
    "def auto_preproc (file_source, problem_type, target, params = {}):\n",
    "\n",
    "    # Gather\n",
    "    df = df_gather(file_source)\n",
    "\n",
    "    # Objective\n",
    "    df_object = df_objective(problem_type, target)\n",
    "\n",
    "    # Describe\n",
    "    df_descr = df_describe(df, df_object, 0.1)\n",
    "\n",
    "    # Clean\n",
    "    df_cleaned = df_clean(df, df_descr, df_object, 0.7, 20)\n",
    "\n",
    "    # Describe | Cleaned\n",
    "    df_descr_cleaned = df_describe(df_cleaned, df_object, 0.1)\n",
    "\n",
    "    # Feature Engineer\n",
    "    df_f_eng = df_feature_eng(df_cleaned, df_descr_cleaned)\n",
    "\n",
    "    # Describe | Feature Engineered\n",
    "    df_descr_f_eng = df_describe(df_f_eng, df_object, 0.1)\n",
    "\n",
    "    return {\n",
    "        'df' : df,\n",
    "        'descr' : df_descr, \n",
    "        'cleaned' : df_cleaned, \n",
    "        'descr_cleaned' : df_descr_cleaned, \n",
    "        'f_eng' : df_f_eng, \n",
    "        'descr_f_eng' : df_descr_f_eng \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Data Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Titanic\n",
    "tn = auto_preproc('data/sample_data/titanic/train.csv', 'classification', 'Survived')\n",
    "\n",
    "# House Prices\n",
    "hp = auto_preproc('data/sample_data/house_price_data/train.csv', 'classification', 'SalePrice')\n",
    "\n",
    "# Wasabi\n",
    "# ws = auto_preproc('data/sample_data/Wasabi/batterytesters_dataset.json', 'classification', 'commonalarm')\n",
    "\n",
    "# Credit Fraud\n",
    "# cc = auto_preproc('data/sample_data/credit_card_fraud/creditcard.csv', 'classification', 'Class')\n",
    "\n",
    "# Modcloth | TOCHECK\n",
    "# file_path_md = 'data/sample_data/modcloth/modcloth_final_data.json'\n",
    "# df_md = pd.read_json(file_path_md, lines=True)\n",
    "\n",
    "# Rain Australia\n",
    "# ra = auto_preproc('data/sample_data/rainAustralia/weatherAUS.csv', 'classification', 'RainTomorrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- S -->\n",
    "train, test = spli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(df, target):\n",
    "\n",
    "    # Split Train, Test\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Model\n",
    "    model = LogisticRegression(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict \n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Validation Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.79\n"
     ]
    }
   ],
   "source": [
    "logistic_regression(tn['f_eng'], 'Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Name_miss</th>\n",
       "      <th>Name_mr</th>\n",
       "      <th>Name_mrs</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.136235</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187172</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0.249214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.175073</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.174136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>0.247206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>0.198911</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass        Age  SibSp  Parch     Fare  \\\n",
       "0              1         0       3  22.000000      1      0   7.2500   \n",
       "1              2         1       1  38.000000      1      0  71.2833   \n",
       "2              3         1       3  26.000000      0      0   7.9250   \n",
       "3              4         1       1  35.000000      1      0  53.1000   \n",
       "4              5         0       3  35.000000      0      0   8.0500   \n",
       "..           ...       ...     ...        ...    ...    ...      ...   \n",
       "886          887         0       2  27.000000      0      0  13.0000   \n",
       "887          888         1       1  19.000000      0      0  30.0000   \n",
       "888          889         0       3  29.699118      1      2  23.4500   \n",
       "889          890         1       1  26.000000      0      0  30.0000   \n",
       "890          891         0       3  32.000000      0      0   7.7500   \n",
       "\n",
       "     Name_miss   Name_mr  Name_mrs  Sex_female  Sex_male  Embarked_C  \\\n",
       "0     0.000000  0.136235  0.000000           0         1           0   \n",
       "1     0.000000  0.000000  0.187172           1         0           1   \n",
       "2     0.249214  0.000000  0.000000           1         0           0   \n",
       "3     0.000000  0.000000  0.175073           1         0           0   \n",
       "4     0.000000  0.174136  0.000000           0         1           0   \n",
       "..         ...       ...       ...         ...       ...         ...   \n",
       "886   0.000000  0.000000  0.000000           0         1           0   \n",
       "887   0.247206  0.000000  0.000000           1         0           0   \n",
       "888   0.198911  0.000000  0.000000           1         0           0   \n",
       "889   0.000000  0.133492  0.000000           0         1           1   \n",
       "890   0.000000  0.163579  0.000000           0         1           0   \n",
       "\n",
       "     Embarked_Q  Embarked_S  \n",
       "0             0           1  \n",
       "1             0           0  \n",
       "2             0           1  \n",
       "3             0           1  \n",
       "4             0           1  \n",
       "..          ...         ...  \n",
       "886           0           1  \n",
       "887           0           1  \n",
       "888           0           1  \n",
       "889           0           0  \n",
       "890           1           0  \n",
       "\n",
       "[891 rows x 15 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn['f_eng']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
